{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "hype = X_train * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires_grad는 해당 tensor에서 모든 연산을 적한 후, backward에서 gradient를 자동으로 계산함  \n",
    "detach를 사용하여 연산 기록으로부터 분리할 수 있음  \n",
    "혹은 valid set에서는 with torch.no_grad()를 사용할 수 있음  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_cost = torch.mean((hype - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr = 0.01)\n",
    "optimizer.zero_grad()\n",
    "MSE_cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 0.04817060008645058\n",
      "cost : 0.029766537249088287\n",
      "cost : 0.018393920734524727\n",
      "cost : 0.011366334743797779\n",
      "cost : 0.007023668382316828\n",
      "cost : 0.00434019835665822\n",
      "cost : 0.0026819754857569933\n",
      "cost : 0.0016572937602177262\n",
      "cost : 0.0010241125710308552\n",
      "cost : 0.0006328357267193496\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    hype = X_train*W + b\n",
    "    cost = torch.mean((hype - y_train) ** 2)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"cost : {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "W = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1/10, W : 0.000, Cost : 18.667\n",
      "Epoch :    2/10, W : 2.800, Cost : 2.987\n",
      "Epoch :    3/10, W : 1.680, Cost : 0.478\n",
      "Epoch :    4/10, W : 2.128, Cost : 0.076\n",
      "Epoch :    5/10, W : 1.949, Cost : 0.012\n",
      "Epoch :    6/10, W : 2.020, Cost : 0.002\n",
      "Epoch :    7/10, W : 1.992, Cost : 0.000\n",
      "Epoch :    8/10, W : 2.003, Cost : 0.000\n",
      "Epoch :    9/10, W : 1.999, Cost : 0.000\n",
      "Epoch :   10/10, W : 2.001, Cost : 0.000\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "n_epoch = 10\n",
    "\n",
    "for epoch in range(1, n_epoch +1):\n",
    "    hype = X_train * W\n",
    "    cost = torch.mean((hype - y_train)**2)\n",
    "    gradient = torch.sum((W*X_train - y_train) * X_train)\n",
    "    print(f\"Epoch : {epoch:4d}/{n_epoch}, W : {W.item():.3f}, Cost : {cost.item():.3f}\")\n",
    "    W -= lr * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1/10, W : 0.000, Cost : 18.667\n",
      "Epoch :    2/10, W : 2.800, Cost : 2.987\n",
      "Epoch :    3/10, W : 1.680, Cost : 0.478\n",
      "Epoch :    4/10, W : 2.128, Cost : 0.076\n",
      "Epoch :    5/10, W : 1.949, Cost : 0.012\n",
      "Epoch :    6/10, W : 2.020, Cost : 0.002\n",
      "Epoch :    7/10, W : 1.992, Cost : 0.000\n",
      "Epoch :    8/10, W : 2.003, Cost : 0.000\n",
      "Epoch :    9/10, W : 1.999, Cost : 0.000\n",
      "Epoch :   10/10, W : 2.001, Cost : 0.000\n"
     ]
    }
   ],
   "source": [
    "lr = 0.15\n",
    "W = torch.zeros(1, requires_grad = True)\n",
    "optimizer = optim.SGD([W], lr = lr)\n",
    "\n",
    "n_epoch = 10\n",
    "\n",
    "for epoch in range(1, n_epoch +1):\n",
    "    hype = X_train * W\n",
    "    cost = torch.mean((hype - y_train)**2)\n",
    "    print(f\"Epoch : {epoch:4d}/{n_epoch}, W : {W.item():.3f}, Cost : {cost.item():.3f}\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## require_grad, zero_grad, no_grad\n",
    "  \n",
    "- require_grad\n",
    "    - True\n",
    "        - Tensor의 gradient 자동 계산, Tensor 내에 저장\n",
    "        - backward 함수를 통해 호출 가능 \n",
    "    - False\n",
    "        - Tensor의 변화도 계산하지 않음\n",
    "- zero_grad\n",
    "    - gradient를 0으로 만듬\n",
    "    - backward를 호출할 때마다 누적되는 것 방지\n",
    "    - 초기화\n",
    "- no_grad\n",
    "    - history 트래킹 하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요즘은 Radam, adamW를 사용  \n",
    "adamP 네이버에서 만듬  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Gradient Descent\n",
    "- 전체 데이터가 아니라 minibatch에 있는 데이터만 사용\n",
    "    - 전체 데이터를 쓰지 않아 잘못된 방향으로 업데이트할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logistic의 연장성\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2e13ac63e90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor([1,2,3])\n",
    "y_pred = F.softmax(z, dim = 0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "$H(P, Q) = -E[logQ(x)] = - \\sum P(x) log Q(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
